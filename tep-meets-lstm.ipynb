{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "tep-meets-lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmxavier/TEP-meets-LSTM/blob/master/tep-meets-lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqslGlEDVDr1"
      },
      "source": [
        "## Step 0 - Setup and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unakqQO5VDr6"
      },
      "source": [
        "# Setup\n",
        "\n",
        "# NOTE: Uncomment the lines bellow in order to run the notebook in Colab (RECOMMENDED)\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/', force_remount=True) # follow the instructions to get the key\n",
        "#%cd drive\n",
        "#%cd MyDrive\n",
        "#!git clone https://github.com/gmxavier/TEP-meets-LSTM.git # clone the repo\n",
        "#%cd TEP-meets-LSTM\n",
        "#!ls # check the repo folder contents\n",
        "#%tensorflow_version 1.x # set the Colab tf version\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "import os\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "# Normalised input features\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\"XMV(1)\", \n",
        "                      \"XMV(2)\", \n",
        "                      \"XMV(3)\", \n",
        "                      \"XMV(4)\", \n",
        "                      \"XMV(5)\", \n",
        "                      \"XMV(6)\", \n",
        "                      \"XMV(7)\", \n",
        "                      \"XMV(8)\", \n",
        "                      \"XMV(9)\", \n",
        "                      \"XMV(10)\", \n",
        "                      \"XMV(11)\"]\n",
        "\n",
        "\n",
        "# Output classes\n",
        "\n",
        "LABELS = [\"NORMAL \", \n",
        "          \"FAULT 1\", \n",
        "          \"FAULT 2\", \n",
        "          \"FAULT 3\", \n",
        "          \"FAULT 4\", \n",
        "          \"FAULT 5\",\n",
        "          \"FAULT 7\"]\n",
        "\n",
        "\n",
        "# Input folders paths\n",
        "\n",
        "DATA_PATH = \"tep/input/\"\n",
        "DATASET_PATH = DATA_PATH + \"tep_dataset/\"\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + signal + \".txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + signal + \".txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"idv.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"idv.txt\"\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def load_X(X_signals_paths):\n",
        "    # Function returns the input features tensor.\n",
        "    X_signals = []\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "    \n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "\n",
        "def load_y(y_path):\n",
        "    # Function returns the fault labels vector.\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.split(' ') for row in file\n",
        "        ]], \n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "    \n",
        "    return y_\n",
        "\n",
        "\n",
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
        "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n",
        "    # Note, some code of this notebook is inspired from an slightly different \n",
        "    # RNN architecture used on another dataset, some of the credits goes to \n",
        "    # \"aymericdamien\" under the MIT license.\n",
        "\n",
        "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
        "    # input shape: (batch_size, n_steps, n_input)\n",
        "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
        "    # Reshape to prepare input to hidden activation\n",
        "    _X = tf.reshape(_X, [-1, n_input]) \n",
        "    # new shape: (n_steps*batch_size, n_input)\n",
        "    \n",
        "    # Linear activation\n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, n_steps, 0) \n",
        "    # new shape: n_steps * (batch_size, n_hidden)\n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    # Get LSTM cell output\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # Get last time step's output feature for a \"many to one\" style classifier, \n",
        "    # as in the image describing RNNs at the top of this page\n",
        "    lstm_last_output = outputs[-1]\n",
        "    \n",
        "    # Linear activation\n",
        "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
        "\n",
        "\n",
        "def extract_batch_size(_train, step, batch_size):\n",
        "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
        "    \n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        index = ((step-1)*batch_size + i) % len(_train)\n",
        "        batch_s[i] = _train[index] \n",
        "\n",
        "    return batch_s\n",
        "\n",
        "\n",
        "def one_hot(y_):\n",
        "    # Function to encode output labels from number indexes \n",
        "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "    \n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = int(np.max(y_)) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
        "\n",
        "\n",
        "def model_size():\n",
        "    # Function to print the number of trainable variables\n",
        "    \n",
        "    size = lambda v: reduce(lambda x, y: x*y, v.get_shape().as_list())\n",
        "    n = sum(size(v) for v in tf.trainable_variables())\n",
        "    print(\"Overall model size: %d\" % (n,))\n",
        "\n",
        "    \n",
        "def parameter_size():\n",
        "    # Function to print the size of trainable variables\n",
        "    \n",
        "    print(\"Parameters sizes:\")\n",
        "    for tf_var in tf.trainable_variables():\n",
        "        print(tf_var.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YBkTnC7VDr-"
      },
      "source": [
        "## Step 1 - Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noh8kNtWVDsA"
      },
      "source": [
        "# Input features tensors\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "\n",
        "# Fault labels\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n",
        "print(\"\")\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print('Faults distribution in the training set:')\n",
        "print(np.asarray((unique_elements, counts_elements)))\n",
        "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
        "print('Faults distribution in the test set:')\n",
        "print(np.asarray((unique_elements, counts_elements)))\n",
        "\n",
        "\n",
        "# Input tensor data \n",
        "\n",
        "training_data_count = len(X_train)  # 5733 training sequences (with 50% overlap between each sequence)\n",
        "test_data_count = len(X_test)  # 2458 testing sequences\n",
        "n_steps = len(X_train[0])  # 128 timesteps per sequence\n",
        "n_input = len(X_train[0][0])  # 11 input features per timestep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjPsvHNrVDsD"
      },
      "source": [
        "## Step 2 - Build the LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsOopZATVDsE"
      },
      "source": [
        "# LSTM internal structure\n",
        "\n",
        "n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 8 # Total classes (due one-hot-encode it should be 8 not 7, \n",
        "              #                as fault 6 is omitted)\n",
        "\n",
        "\n",
        "# Training hyperparameters\n",
        "\n",
        "learning_rate = 0.0025\n",
        "lambda_loss_amount = 0.0015\n",
        "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
        "batch_size = 1500\n",
        "display_iter = 30000  # To show test set accuracy during training\n",
        "\n",
        "\n",
        "# Graph input/output\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "\n",
        "# Graph weights\n",
        "\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "\n",
        "# Loss, optimizer and evaluation\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
        ") # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8TIwc2EVDsG"
      },
      "source": [
        "## Step 3 - Train the LSTM network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnNZiwXZVDsH"
      },
      "source": [
        "# To keep track of training's performance\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "X_ = np.append(X_train, X_test, axis=0)\n",
        "y_ = np.append(y_train, y_test, axis=0)\n",
        "\n",
        "nfold = 5\n",
        "dr = []\n",
        "ks = np.array_split(np.arange(len(y_)), nfold)\n",
        "    \n",
        "for k in ks:\n",
        "    \n",
        "    # Launch the graph\n",
        "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Some useful info\n",
        "    print(\"Some useful info ...\")\n",
        "    model_size()\n",
        "    parameter_size()\n",
        "    print(\"\")\n",
        "    print(\"Starting training ...\")\n",
        "\n",
        "    # Perform Training steps with \"batch_size\" amount of example data at each loop\n",
        "    step = 1\n",
        "    while step * batch_size <= training_iters:\n",
        "        batch_xs =         extract_batch_size(np.delete(X_, k, axis=0), step, batch_size)\n",
        "        batch_ys = one_hot(extract_batch_size(np.delete(y_, k, axis=0), step, batch_size))\n",
        "\n",
        "        # Fit training using batch data\n",
        "        _, loss, acc = sess.run(\n",
        "            [optimizer, cost, accuracy],\n",
        "            feed_dict={\n",
        "                x: batch_xs, \n",
        "                y: batch_ys\n",
        "            }\n",
        "        )\n",
        "        train_losses.append(loss)\n",
        "        train_accuracies.append(acc)\n",
        "    \n",
        "        # Evaluate network only at some steps for faster training: \n",
        "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
        "            \n",
        "            # To not spam console, show training accuracy/loss in this \"if\"\n",
        "            print(\"Iteration #\" + str(step*batch_size) + \"\\n\" + \\\n",
        "                  \"TRAINING SET: \" + \\\n",
        "                  \"Batch Loss = {:.6f}\".format(loss) + \\\n",
        "                  \", Accuracy = {:.6f}\".format(acc))\n",
        "        \n",
        "            # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
        "            loss, acc = sess.run(\n",
        "                [cost, accuracy], \n",
        "                feed_dict={\n",
        "                    x: X_[k],\n",
        "                    y: one_hot(y_)[k]\n",
        "                }\n",
        "            )\n",
        "            test_losses.append(loss)\n",
        "            test_accuracies.append(acc)\n",
        "            print(\"    TEST SET: \" + \\\n",
        "                  \"Batch Loss = {:.6f}\".format(loss) + \\\n",
        "                  \", Accuracy = {:.6f}\".format(acc))\n",
        "    \n",
        "        step += 1\n",
        "\n",
        "    print(\"Optimization finished!\")\n",
        "\n",
        "    # Accuracy for test data\n",
        "\n",
        "    one_hot_predictions, final_acc, final_loss = sess.run(\n",
        "        [pred, accuracy, cost],\n",
        "        feed_dict={\n",
        "            x: X_[k],\n",
        "            y: one_hot(y_)[k]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    test_losses.append(final_loss)\n",
        "    test_accuracies.append(final_acc)\n",
        "    \n",
        "    print(\"FINAL RESULT: \" + \\\n",
        "          \"Batch Loss = {:.6f}\".format(final_loss) + \\\n",
        "          \", Accuracy = {:.6f}\".format(final_acc))\n",
        "    \n",
        "    predictions = one_hot_predictions.argmax(1)\n",
        "    aux = metrics.confusion_matrix(y_[k], predictions, labels = np.unique(y_))\n",
        "    dr.append(100*aux.diagonal()/(np.sum(aux, axis = 1)+1e-12))\n",
        "    \n",
        "    print(\"Cross-validation fold #\" + str(len(dr)) + \" of \" + str(nfold))\n",
        "    \n",
        "    sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPQRWqObVDsJ"
      },
      "source": [
        "## Step 4 - Plot the training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkW8aX1VDsK"
      },
      "source": [
        "# (Inline plots: )\n",
        "%matplotlib inline\n",
        "\n",
        "font = {\n",
        "    'family' : 'Bitstream Vera Sans',\n",
        "    'weight' : 'bold',\n",
        "    'size'   : 18\n",
        "}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
        "plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
        "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
        "\n",
        "indep_test_axis = np.append(\n",
        "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
        "    [training_iters]\n",
        ")\n",
        "plt.plot(indep_test_axis, np.array(test_losses),     \"b-\", label=\"Test losses\")\n",
        "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
        "\n",
        "plt.title(\"Training session's progress over iterations and folds\")\n",
        "plt.legend(loc='upper right', shadow=True)\n",
        "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
        "plt.xlabel('Training iteration')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm3BriqCVDsL"
      },
      "source": [
        "## Step 5 - Print and plot the final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fakT5M95VDsM"
      },
      "source": [
        "# Print results\n",
        "\n",
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing accuracy: {:.2f}%\".format(100*final_acc))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {:.2f}%\".format(100*metrics.precision_score(y_[k], predictions, average=\"weighted\")))\n",
        "print(\"Recall: {:.2f}%\".format(100*metrics.recall_score(y_[k], predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {:.2f}%\".format(100*metrics.f1_score(y_[k], predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion matrix:\")\n",
        "confusion_matrix = metrics.confusion_matrix(y_[k], predictions)\n",
        "print(confusion_matrix)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "print(np.array_str(normalised_confusion_matrix, precision=2, suppress_small=True))\n",
        "\n",
        "\n",
        "# Plot results \n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "res = plt.imshow(np.array(confusion_matrix), cmap=plt.cm.summer, interpolation='nearest')\n",
        "for i, row in enumerate(confusion_matrix):\n",
        "    for j, c in enumerate(row):\n",
        "        if c>0:\n",
        "            plt.text(j-.2, i+.1, c, fontsize=16)\n",
        "            \n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "_ = plt.xticks(range(n_classes), [l for l in LABELS], rotation=90)\n",
        "_ = plt.yticks(range(n_classes), [l for l in LABELS])\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}